{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuan YU\n",
    "# Word2vec & Semantic Similarity Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. For any one selected article title from the dataset, finds 10 most similar titles (excluding the selected article title) based on Word2Vec similarity, prints those titles and their similarity scores in a descending order of similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, operator\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data and select a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "json = pd.read_json(\"webhose_netflix-Copy1.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25288"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=[]\n",
    "for i in range(len(json)):\n",
    "    documents.append(json.title[i])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate the dataset by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduptitle=[]\n",
    "dedupdocu=[]\n",
    "for rows in json.itertuples():\n",
    "    if len(rows.title)!=0:\n",
    "        if rows.title not in list(set(deduptitle)):\n",
    "            r=rows\n",
    "            dedupdocu.append(r)\n",
    "            deduptitle.append(rows.title)\n",
    "\n",
    "len(dedupdocu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0        13 Reasons Why: The popular Netflix show's cre...\n",
       "1        Judge gives control of 'Tiger King' Joe Exotic...\n",
       "2        A TV reboot of Bong Joon-ho's acclaimed film S...\n",
       "3        2-Pack: Ideaworks Mosquito Killer Lamps (batte...\n",
       "4        Already-Obese Average Americans Have Drunk & E...\n",
       "                               ...                        \n",
       "19508    Jerry Seinfeld channels James Bond in Netflix ...\n",
       "19509    Why you should embrace the quarantine boredom:...\n",
       "19510    SHE-RA E AS PRINCESAS DO PODER: Trailer Final ...\n",
       "19511        7 Netflix shows canceled way too soon, ranked\n",
       "19512           New on Netflix, HBO, Hulu, Amazon May 2020\n",
       "Name: title, Length: 19513, dtype: object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupdocu=pd.DataFrame(dedupdocu)\n",
    "dedupdocu['title'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the word2vec pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the downloaded binary\n",
    "filepath = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(filepath, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prepare the unique title list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduptitle=list(set(deduptitle))\n",
    "titles=[]\n",
    "for t in deduptitle:\n",
    "    titles.append({'text':t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Martin Scorsese's next film to feature Leonardo DiCaprio, Robert De Niro\"},\n",
       " {'text': 'What To Do If Your Employer Has Stopped Matching Your 401k Contributions'},\n",
       " {'text': 'Reflecting on Luke Cage, the Pioneering Black Superhero, and His Lasting Legacy'},\n",
       " {'text': 'HBO is taking on Netflix with human curation instead of relying on algorithms'},\n",
       " {'text': '20:15 | Das Erste | Naturwunder Okawango'},\n",
       " {'text': 'Anurag Kashyap’s Choked to Launch in June on Netflix – The News Of India'},\n",
       " {'text': \"Despite Trump's Demand, Not All Houses Of Worship Reopen\"},\n",
       " {'text': 'Learn how to decorate cakes like a pro'},\n",
       " {'text': '’13 Reasons Why’ Sets Final Season Premiere Date | Decider'},\n",
       " {'text': 'Netflix announces the launch date for Undercover season 2.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank texts based on similiarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokens are included in the vocabulary\n",
    "import re\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '',text)\n",
    "    text = [word for word in text.split() if word in model.key_to_index]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Martin',\n",
       " 'Scorseses',\n",
       " 'next',\n",
       " 'film',\n",
       " 'feature',\n",
       " 'Leonardo',\n",
       " 'DiCaprio',\n",
       " 'Robert',\n",
       " 'De',\n",
       " 'Niro']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = clean(titles[0]['text'])\n",
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in titles:\n",
    "    input2 = clean(t['text'])\n",
    "#some of the titles are empty after cleaning\n",
    "    if len(input2)!= 0:\n",
    "        t['sim'] = model.n_similarity(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Martin Scorsese's next film to feature Leonardo DiCaprio, Robert De Niro\",\n",
       "  'sim': 1.0},\n",
       " {'text': 'What To Do If Your Employer Has Stopped Matching Your 401k Contributions',\n",
       "  'sim': 0.19035427},\n",
       " {'text': 'Reflecting on Luke Cage, the Pioneering Black Superhero, and His Lasting Legacy',\n",
       "  'sim': 0.4287738},\n",
       " {'text': 'HBO is taking on Netflix with human curation instead of relying on algorithms',\n",
       "  'sim': 0.30790216},\n",
       " {'text': '20:15 | Das Erste | Naturwunder Okawango', 'sim': 0.18395117},\n",
       " {'text': 'Anurag Kashyap’s Choked to Launch in June on Netflix – The News Of India',\n",
       "  'sim': 0.30300277},\n",
       " {'text': \"Despite Trump's Demand, Not All Houses Of Worship Reopen\",\n",
       "  'sim': 0.14312756},\n",
       " {'text': 'Learn how to decorate cakes like a pro', 'sim': 0.22588474},\n",
       " {'text': '’13 Reasons Why’ Sets Final Season Premiere Date | Decider',\n",
       "  'sim': 0.23171097},\n",
       " {'text': 'Netflix announces the launch date for Undercover season 2.',\n",
       "  'sim': 0.32902905}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar=[]\n",
    "for t in titles:\n",
    "    if len(clean(t['text']))!=0:\n",
    "        similar.append(t)\n",
    "similar[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting using the built-in **sorted** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " 0.9616291 \n",
      " Leonardo DiCaprio, Robert De Niro to star in Martin Scorsese's next film\n",
      "2 \n",
      " 0.8403441 \n",
      " Apple seals pact for Martin Scorsese's $200 million film starring Leonardo DiCaprio, De Niro\n",
      "3 \n",
      " 0.80540144 \n",
      " Apple secures new Scorsese film starring DiCaprio, De Niro\n",
      "4 \n",
      " 0.7868109 \n",
      " Apple secures deal for Scorsese’s next film starring DiCaprio, De Niro\n",
      "5 \n",
      " 0.7793726 \n",
      " Apple reportedly nabs Scorcese film starring DiCaprio and De Niro\n",
      "6 \n",
      " 0.77775156 \n",
      " Apple lands Martin Scorsese movie Killers of the Flower Moon starring De Niro and DiCaprio\n",
      "7 \n",
      " 0.77384615 \n",
      " Martin Scorsese’s ‘Killers of the Flower Moon’ Starring Leonardo DiCaprio, Robert De Niro To Be An Apple Movie\n",
      "8 \n",
      " 0.7671393 \n",
      " Apple secures deal for Scorsese’s next film starring DiCaprio, De Niro: media\n",
      "9 \n",
      " 0.7671393 \n",
      " Apple secures deal for Scorsese's next film starring DiCaprio, De Niro -media\n",
      "10 \n",
      " 0.7671393 \n",
      " Apple secures deal for Scorsese's next film starring DiCaprio, De Niro: media\n"
     ]
    }
   ],
   "source": [
    "similar=sorted(similar, key=lambda s: s['sim'],reverse=True)\n",
    "#exclude the similar[0] which should be the original title\n",
    "for s in similar[1:11]:\n",
    "    print(similar.index(s),'\\n',s['sim'],'\\n',s['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleans up and tokenizes article bodies using the RegexTokenizer and Stopword remover functions provided in the Class ExerciseDownload Class Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 18:41:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/20 18:41:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/20 18:41:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads previously deduplicated obtained dataset into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Nested StructType not supported in conversion from Arrow: struct<country: string, domain_rank: int64, main_image: string, participants_count: int64, performance_score: int64, published: string, replies_count: int64, section_title: string, site: string, site_categories: list<item: string>, site_full: string, site_section: string, site_type: string, social: struct<facebook: struct<comments: int64, likes: int64, shares: int64>, gplus: struct<shares: int64>, linkedin: struct<shares: int64>, pinterest: struct<shares: int64>, stumbledupon: struct<shares: int64>, vk: struct<shares: int64>>, spam_score: double, title: string, title_full: string, url: string, uuid: string>\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(dedupdocu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: long (nullable = true)\n",
      " |-- thread: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- ord_in_thread: long (nullable = true)\n",
      " |-- parent_url: double (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- published: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- highlightText: string (nullable = true)\n",
      " |-- highlightTitle: string (nullable = true)\n",
      " |-- highlightThreadTitle: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- external_links: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_images: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- entities: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- crawled: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 18:41:41 WARN TaskSetManager: Stage 0 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 18:41:46 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "+--------------------+--------------------+\n",
      "|               title|                text|\n",
      "+--------------------+--------------------+\n",
      "|13 Reasons Why: T...|The controversial...|\n",
      "|Judge gives contr...|A federal judge i...|\n",
      "|A TV reboot of Bo...|WhatsApp If you e...|\n",
      "|2-Pack: Ideaworks...|A Real Buzz-Worth...|\n",
      "|Already-Obese Ave...|by Tyler Durden T...|\n",
      "|Netflix, Disney j...|Hollywood is voic...|\n",
      "|Novel Entertainme...|Novel Entertainme...|\n",
      "|Anime Based On Be...|2 min read share\\...|\n",
      "|Tiger King star C...|Updated : 3 Jun 2...|\n",
      "|All about Netflix...|All about Netflix...|\n",
      "|File:Federation s...|A Federation stat...|\n",
      "|news: Reemerged d...|QUEEN ELIZABETH I...|\n",
      "|Samsung TV Plus L...|The niche LGBTQ+ ...|\n",
      "|US Says Will Inve...|Washington, Jun 3...|\n",
      "|COMPACT PRO G3 | ...|Display Technolog...|\n",
      "|Even with Masterc...|Glenn Dyer's TV R...|\n",
      "|Who Is Abigail Ko...|Who Is Abigail Ko...|\n",
      "|Egypt- Legendary ...|(MENAFN - Daily N...|\n",
      "|“Hollywood” Gives...|Set after WWII in...|\n",
      "|Data centre stock...|Passengers on one...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('title','text').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer to tokenize inputText into individual tokens (words)\n",
    "regextok = RegexTokenizer(gaps = False, pattern = '\\w+', \n",
    "                          inputCol = 'text', outputCol = 'tokens')\n",
    "# StopWordsRemover to remove stopwords in the list of tokens\n",
    "stopwrmv = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')\n",
    "\n",
    "df = regextok.transform(df)\n",
    "df = stopwrmv.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 01:52:50 WARN TaskSetManager: Stage 2 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 01:52:54 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 2): Attempting to kill Python Worker\n",
      "+--------------------+--------------------+--------------------+\n",
      "|   tokens_sw_removed|              tokens|                text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[controversial, 1...|[the, controversi...|The controversial...|\n",
      "|[federal, judge, ...|[a, federal, judg...|A federal judge i...|\n",
      "|[whatsapp, enjoye...|[whatsapp, if, yo...|WhatsApp If you e...|\n",
      "|[real, buzz, wort...|[a, real, buzz, w...|A Real Buzz-Worth...|\n",
      "|[tyler, durden, t...|[by, tyler, durde...|by Tyler Durden T...|\n",
      "|[hollywood, voici...|[hollywood, is, v...|Hollywood is voic...|\n",
      "|[novel, entertain...|[novel, entertain...|Novel Entertainme...|\n",
      "|[2, min, read, sh...|[2, min, read, sh...|2 min read share\\...|\n",
      "|[updated, 3, jun,...|[updated, 3, jun,...|Updated : 3 Jun 2...|\n",
      "|[netflix, sci, fi...|[all, about, netf...|All about Netflix...|\n",
      "|[federation, stat...|[a, federation, s...|A Federation stat...|\n",
      "|[queen, elizabeth...|[queen, elizabeth...|QUEEN ELIZABETH I...|\n",
      "|[niche, lgbtq, st...|[the, niche, lgbt...|The niche LGBTQ+ ...|\n",
      "|[washington, jun,...|[washington, jun,...|Washington, Jun 3...|\n",
      "|[display, technol...|[display, technol...|Display Technolog...|\n",
      "|[glenn, dyer, tv,...|[glenn, dyer, s, ...|Glenn Dyer's TV R...|\n",
      "|[abigail, koppel,...|[who, is, abigail...|Who Is Abigail Ko...|\n",
      "|[menafn, daily, n...|[menafn, daily, n...|(MENAFN - Daily N...|\n",
      "|[set, wwii, late,...|[set, after, wwii...|Set after WWII in...|\n",
      "|[passengers, one,...|[passengers, on, ...|Passengers on one...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('tokens_sw_removed','tokens','text').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trains a Word2Vec model based on the output column produced in step 2. Implements any sample search query, which can be any combination of words of interest, and produces the top 10 matching article titles among all article titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vectorSize = 100, minCount = 3, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors',seed=1, \n",
    "                   windowSize= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 02:13:01 WARN TaskSetManager: Stage 13 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 02:13:05 WARN TaskSetManager: Stage 15 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model2 = word2vec.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(dedupdocu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = regextok.transform(df2)\n",
    "df2 = stopwrmv.transform(df2)\n",
    "df2 = model2.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 02:30:44 WARN TaskSetManager: Stage 18 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19513"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 03:21:31 WARN TaskSetManager: Stage 47 contains a task of very large size (12222 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chunks = df2.select('title','wordvectors').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(title=\"13 Reasons Why: The popular Netflix show's creator teases chance of a hopeful ending\", wordvectors=DenseVector([0.0733, -0.0365, -0.0192, -0.0049, -0.0898, -0.0469, 0.0029, -0.0093, 0.0084, -0.0431, -0.0397, -0.1332, -0.0714, -0.117, 0.0003, 0.0742, 0.0286, 0.0355, 0.0543, -0.0168, -0.0051, -0.0149, -0.0326, 0.0291, -0.0325, -0.0434, 0.0509, -0.0473, 0.0637, -0.0398, 0.0206, 0.0923, 0.023, 0.0942, 0.0025, 0.0285, -0.0041, 0.0294, -0.0556, 0.0018, 0.0186, 0.0876, 0.0192, 0.0023, -0.0696, -0.1378, -0.0169, 0.051, -0.0152, 0.0017, 0.0951, 0.0881, -0.0551, -0.0363, -0.0587, -0.0044, 0.0273, -0.0185, -0.0237, -0.0588, -0.0161, -0.0195, -0.0555, 0.0765, 0.028, -0.0179, 0.0115, 0.0661, -0.0215, 0.1255, 0.0055, 0.053, -0.1649, 0.0028, -0.0036, -0.0213, 0.0683, -0.0873, -0.0541, 0.0059, -0.0469, 0.0192, -0.1632, 0.0051, 0.0362, 0.0126, 0.043, 0.0292, 0.0131, -0.0475, -0.1018, 0.0309, -0.0865, -0.0486, 0.0521, 0.0286, 0.0157, -0.0175, 0.1242, 0.0043]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search query in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEARCH_QUERY(text):\n",
    "    query_df = spark.createDataFrame([(1, text)]).toDF('index','text')\n",
    "    query_tok = regextok.transform(query_df)\n",
    "    query_swr = stopwrmv.transform(query_tok)\n",
    "    query_vec = model2.transform(query_swr)\n",
    "    query_vec.show()\n",
    "    query_vec = query_vec.select('wordvectors').collect()[0][0]\n",
    "    return(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+-----------------+--------------------+\n",
      "|index|      text|        tokens|tokens_sw_removed|         wordvectors|\n",
      "+-----+----------+--------------+-----------------+--------------------+\n",
      "|    1|I love NLP|[i, love, nlp]|      [love, nlp]|[0.09419919084757...|\n",
      "+-----+----------+--------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_vec=SEARCH_QUERY(\"I love NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cossim(v1, v2): \n",
    "    \n",
    "    if np.dot(v1, v1) == 0 or np.dot(v2, v2) == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / (np.sqrt(np.dot(v2, v2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "|title                                                                                              |similarity        |\n",
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "|Review: Netflix’s The Half of It queers a tired, age-old love story                                |0.6181124119098784|\n",
      "|ClickTheCity: 'The Half Of It' On Netflix Movie Review                                             |0.6164820355289613|\n",
      "|Netflix’s ‘The Half of It’ Tells a Story About All Kinds of Love                                   |0.6085601570181687|\n",
      "|The Half Of It REVIEW – Coming-of-Age Excellence                                                   |0.595284412371208 |\n",
      "|Why Netflix’s ‘A Secret Love’ Made Me Uncomfortable––And I’m Not The Only One                      |0.5829134552647822|\n",
      "|Director Alice Wu On Her New Film 'The Half Of It'                                                 |0.5809980910405179|\n",
      "|23 Lines From Netflix's 'The Half Of It' That Every Hopeless Romantic Must Read                    |0.5746581132150311|\n",
      "|The very best Normal People-vibe TV shows to fill the Marianne and Connell-shaped hole in our lives|0.5713142760347398|\n",
      "|'The Half of It' Is Netflix's Latest Teen Drama That You'll Fall In Love With!                     |0.5705531828302844|\n",
      "|THE HALF OF IT: A Delightful LGBTQ Tale Of Self-Acceptance & Friendship                            |0.5627893193134993|\n",
      "+---------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(i[0], float(cossim(query_vec, i[1]))) for i in chunks]\n",
    "sim_df = spark.createDataFrame(data).toDF( 'title', 'similarity').orderBy('similarity', ascending=False)\n",
    "sim_df.show(10, truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
